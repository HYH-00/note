# 决策树

通过一系列的因素判断做出一种决策，这种判断背后的逻辑如同一颗树一样，也就是所谓的决策树，非叶子节点是条件，叶子节点是一个决策结果

## 特征选择

决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。

## 构造

构造也就是构造一棵完整的决策树，也就是选择因素作为决策树的节点

决策树有三种节点：根节点、内部节点、叶子节点



开始：构建根节点，将所有训练数据都放在根节点，选择一个最优特征，按着这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。

如果这些子集已经能够被基本正确分类，那么构建叶节点，并将这些子集分到所对应的叶节点去。

如果还有子集不能够被正确的分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的节点，如果递归进行，直至所有训练数据子集被基本正确的分类，或者没有合适的特征为止

每个子集都被分到叶节点上，即都有了明确的类，这样就生成了一颗决策树。

### 纯度：

**你可以把决策树的构造过程理解成为寻找纯净划分的过程**。数学上，我们可以用纯度来表示，纯度换一种方式来解释就是**让目标变量的分歧最小**。

### 信息熵：表示信息的不确定度

<img src="https://i.loli.net/2021/01/27/a7iQpsxCJZBhkn3.png" alt="1411882-20190407121846566-2117307533" style="zoom: 67%;" />

p(i|t) 代表了节点 t 为分类 i 的概率，其中 log2 为取以 2 为底的对数。它能帮我们反映出来这个信息的不确定度。**当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。**

如，一个决策，A情况为5，B情况为1时信息熵为

<img src="https://i.loli.net/2021/01/27/eQzNKUx15SuPLqR.png" alt="1411882-20190407122345219-1548846464" style="zoom:67%;" />

如果A为3，B为3则消息熵为

<img src="https://i.loli.net/2021/01/27/az8Q91l7ScZJBXy.png" alt="1411882-20190407122423731-1318911835" style="zoom:67%;" />

**信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。**



构造决策树的时候，会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是**信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）**

### 信息增益：

信息增益指的就是划分可以带来纯度的提高，信息熵的下降。它的计算公式，是**父亲节点的信息熵减去所有子节点的信息熵**。在计算的过程中，我们会计算每个子节点的归一化信息熵，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为

![1411882-20190407122958568-131890655](https://i.loli.net/2021/01/27/8kl6z2y1TpJt5uK.png)

## ID3算法

1）从根结点(root node)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征。

2）由该特征的不同取值建立子节点，再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止；

3）最后得到一个决策树。

## 在 ID3 算法上进行改进的 C4.5 算法

### 1. 采用信息增益率

因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。**信息增益率 = 信息增益 / 属性熵**

当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。

### 2. 采用悲观剪枝

ID3 构造决策树的时候，容易产生过拟合的情况。在 C4.5中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。

悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。

### 3. 离散化处理连续属性

C4.5 可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，**C4.5 选择具有最高信息增益的划分所对应的阈值**。

### 4. 处理缺失值

针对数据集不完整的情况，C4.5 也可以进行处理。

假如我们得到的是如下的数据，你会发现这个数据中存在两点问题。第一个问题是，数据集中存在数值缺失的情况，如何进行属性选择？第二个问题是，假设已经做了属性划分，但是样本在这个属性上有缺失值，该如何对样本进行划分？

## 剪枝

剪枝也就是剪去不必要的判断因素，而得到不错的结果，提高判断速度也防止过拟合

![画板](https://i.loli.net/2021/01/27/QI7ROrEW5XDef49.png)

**造成过拟合的原因**：

一是因为训练集中样本量较小。如果决策树选择的属性过多，构造出来的决策树一定能够“完美”地把训练集中的样本分类，但是这样就会把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点，这就使得这个决策树在真实的数据分类中出现错误，也就是模型的“泛化能力”差。

**泛化能力**：指的分类器是通过训练集抽象出来的分类能力，你也可以理解是举一反三的能力。如果我们太依赖于训练集的数据，那么得到的决策树容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。

**剪枝的方法**：

- **预剪枝**：在决策树构造时就进行剪枝。方法是，在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。
- **后剪枝**：在生成决策树之后再进行剪枝。通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。



**实现方式**：极小化决策树整体的损失函数或代价函数来实现

![Snipaste_2021-01-27_19-46-32](https://i.loli.net/2021/01/27/Ekt6Bi9sKoNLu7Z.png)

| 参数      | 意义                                                         |
| --------- | ------------------------------------------------------------ |
| T         | 表示这棵子树的叶子节点，                                     |
| H t ( T ) | 表示第t 个叶子的熵，![这里写图片描述](https://img-blog.csdn.net/20180314091812955?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) |
| N t       | 表示该叶子所含的训练样例的个数，                             |
| α         | 惩罚系数，                                                   |
| $         | T                                                            |

## 小结

首先 ID3 算法的优点是方法简单，缺点是对噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误。C4.5 在 IID3 的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝、处理连续数值以及数值缺失等情况，但是由于 C4.5 需要对数据集进行多次扫描，算法效率相对较低。